# Bandit Simulation

Simulation framework for multi-armed bandit experiments with hypothesis testing, used in the paper **A Statistically Reliable Optimization Framework for Bandit Experiments in Scientific Discovery**.

---

## Setup

```bash
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
pip install -e .
```

---

## Project Structure

```
src/                 Core library (bandit algorithms, simulation engine, Bayesian models)
scripts/             Runnable experiment scripts (one per paper setting/table/figure)

figures/             Output figures (generated by scripts)
results/             Output CSVs from experiments
```

---

## Reproducing Paper Results

Each script in `scripts/` corresponds to a table or figure set in the paper. Run from the project root.

---

### Table 1 — Wald Test FPR Under Adaptive Sampling

Analyzes how the Wald test statistic distribution deviates from N(0,1) under Thompson Sampling, and compares false positive rates between a fixed classical threshold (from z-table) and ourproposed algorithm-induced test (AIT) correction.

```bash
python scripts/wald_fpr_analysis.py
```

---

### Table 2 — ART vs AIT for Two-Sample t-tests (Power and FPR)

Compares the empirical power and false positive rate (FPR) of Algorithm-Induced Test correction (AIT) versus Algorithm Replay Test correction (ART) when data are collected adaptively by common bandit algorithms. We consider a two-sample t-test for the composite hypothesis H0: mu1 = mu2 against H1: mu1 != mu2, with total horizon T = 200 and Bernoulli rewards.

```bash
python scripts/ait_art_comparison.py
```

---

### Table 3 & Figure 1 — Empirically Inspired Simulation (Prior vs Post Evaluation)

Illustrates how the proposed optimization framework guides adaptive experiment design under an empirically inspired 6-arm simulation.

Figure 1 shows the relative ECP-reward performance across epsilon-TS settings under the prior (design-time) specification. The visualization allows experimenters to compare epsilon-TS against TS (epsilon = 0) and UR (epsilon = 1) across different experiment extension costs w, and identify the best-performing configuration at a chosen w (e.g., w = 0.01).

Table 3 compares experiment designs under both:

- Prior (design-time): Expected performance under the prior reward distribution used during optimization.
- Post (realized): Realized performance using the empirical arm means.

The table evaluates:

- UR (naive)
- TS (AIT-corrected design)
- epsilon-TS (AIT + optimized epsilon)

at experiment extension cost w = 0.01.

```bash
# Prior (design-time optimization + GUI visualization). We will update the actual GUI web link once it is fully available online. Currently it is under local testing phase.
python scripts/empirical_sim_prior.py

# Post (realized performance evaluation)
python scripts/empirical_sim_post.py
```

---

### Table 4 — Evaluation Across Hypothesis Tests

Evaluates the proposed optimization procedure across multiple hypothesis tests and bandit algorithms under a unified reward–inference tradeoff framework.

All experiments:

- Type I error fixed at 0.05
- Power constraint fixed at 0.8
- Experiment extension cost w = 0.1

We compare:

- Naive UR
- Naive TS
- Naive epsilon-TS (epsilon = 0.5)
- Optimized epsilon-TS (epsilon selected via the proposed optimization procedure)

Across tests:

- ANOVA
- T-Constant
- T-Control
- Tukey

The table reports ECP-reward (higher is better). Results show that applying the optimization procedure consistently improves ECP-reward over fixed designs and avoids substantial suboptimality under certain tests.

```bash
python scripts/sweep_across_tests.py
```

---

### Table 5 — Prior Mis-specification Sensitivity Check

Evaluates robustness of the optimization framework under prior mis-specification.

We consider epsilon-TS optimized under a reference Beta(3.2, 5.9) prior (mean = 0.35, std = 0.15) with:

- ANOVA test
- Type I error = 0.05
- Power constraint = 0.8
- Experiment extension cost w = 0.1

Two types of mis-specification are studied:

1. Location mis-specification (mean varies from 0.1 to 0.5, scale fixed)
2. Scale mis-specification (std varies from 0.09 to 0.21, mean fixed)

The table reports ECP-reward loss relative to the true optimal design at each setting, along with a random-epsilon baseline for comparison. Results show that the optimization framework is robust to moderate prior deviations, with losses substantially smaller than random parameter selection.

```bash
python scripts/mis_specification.py
```
